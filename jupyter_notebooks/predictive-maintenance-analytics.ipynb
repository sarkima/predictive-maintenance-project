{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Engine Predictive Maintenance Analytics\n",
        "\n",
        "This notebook loads the NASA C MAPSS FD001 dataset (Kaggle version), \n",
        "performs preprocessing, builds baseline and improved predictive models, \n",
        "and exports predictions and metrics for the Streamlit dashboard app."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "* The primary objective of the notebook is to predict the Remaining Useful Life (RUL) of a Turbo Fan Engine using Data Analytics and Machine Learning.\n",
        "* The secondary objectives are to use the dataset to answer a number of businness questions and hypotheses by exploring three data analytics categories: descriptive analytics, diagnostic analytics and predictive analytics.\n",
        "* Write your notebook objective here, for example, \"Fetch data from Kaggle and save as raw data\", or \"engineer features for modelling\" (**remove**)\n",
        "\n",
        "### Inputs\n",
        "\n",
        "* The notebook uses a subset (FD001) of the NASA CMAPSS (Commercial Modular Aero Propulsion System Simulation) Turbo Fan Engine Degradation dataset (Kaggle version) to build a predictive maintenance model capable of estimating the Remaining Useful Life (RUL) of engines based on sensor data collected over time.  The FD001 subset is one of four subsets of the dataset, with each subset representing a different engine but of the same class.\n",
        "* Write down which data or information you need to run the notebook (**remove**)\n",
        "\n",
        "### Outputs\n",
        "\n",
        "* The notebook exports predictions and metrics for the Streamlit dashboard app\n",
        "* Code, plots and a predictive model were writen, ploted and built at various stages of the notebook\n",
        "* Write here which files, code or artefacts you generate by the end of the notebook (**remove**) \n",
        "\n",
        "### Additional Comments\n",
        "\n",
        "* If you have any additional comments that don't fit in the previous bullets, please state them here. (**remove later**)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Important Libraries\n",
        "\n",
        "* All the important libraries needed for the project have been loaded to the virtual environment (.venv).\n",
        "* The libraries used for the project are listed in the requirement.txt file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Matplotlib is building the font cache; this may take a moment.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "pd.set_option(\"display.max_columns\", None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Dataset\n",
        "\n",
        "* This loads the dataset on to the Jupyter Notebook\n",
        "* Data Set: FD001\n",
        "  - Train trjectories: 100\n",
        "  - Test trajectories: 100\n",
        "  - Conditions: ONE (Sea Level)\n",
        "  - Fault Modes: ONE (HPC Degradation) \n",
        "  - Dataset subsets:\n",
        "    - train_FD001.txt\n",
        "    - test_FD001.txt\n",
        "    - RUL_FD001.txt\n",
        "    - x.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/train_FD001.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     25\u001b[39m     df_rul = pd.read_csv(\n\u001b[32m     26\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/RUL_FD001.txt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m         sep=\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m         header=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     29\u001b[39m         names=[\u001b[33m\"\u001b[39m\u001b[33mRUL\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     30\u001b[39m     )\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df_train, df_test, df_rul\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m df_train, df_test, df_rul = \u001b[43mload_fd001\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m df_train.head(), df_test.head(), df_rul.head()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mload_fd001\u001b[39m\u001b[34m(data_dir)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_fd001\u001b[39m(data_dir=\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      6\u001b[39m     col_names = [\n\u001b[32m      7\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mengine_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcycle\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mop_setting_1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mop_setting_2\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mop_setting_3\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m     ] + [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msensor_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m22\u001b[39m)]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     df_train = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/train_FD001.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43ms+\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcol_names\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     df_test = pd.read_csv(\n\u001b[32m     19\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/test_FD001.txt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     20\u001b[39m         sep=\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m         header=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     22\u001b[39m         names=col_names\n\u001b[32m     23\u001b[39m     )\n\u001b[32m     25\u001b[39m     df_rul = pd.read_csv(\n\u001b[32m     26\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/RUL_FD001.txt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m         sep=\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m         header=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     29\u001b[39m         names=[\u001b[33m\"\u001b[39m\u001b[33mRUL\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     30\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Documents\\Sarkima\\Sarki\\MyEducation\\MyCourses\\CodeInstitute\\DataAnalytics\\VSCode\\predictive-maintenance-project\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    935\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    936\u001b[39m     dialect,\n\u001b[32m    937\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    944\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    945\u001b[39m )\n\u001b[32m    946\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Documents\\Sarkima\\Sarki\\MyEducation\\MyCourses\\CodeInstitute\\DataAnalytics\\VSCode\\predictive-maintenance-project\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    608\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    610\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    614\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Documents\\Sarkima\\Sarki\\MyEducation\\MyCourses\\CodeInstitute\\DataAnalytics\\VSCode\\predictive-maintenance-project\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1447\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Documents\\Sarkima\\Sarki\\MyEducation\\MyCourses\\CodeInstitute\\DataAnalytics\\VSCode\\predictive-maintenance-project\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1703\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1704\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1705\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1712\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1714\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1715\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1716\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Documents\\Sarkima\\Sarki\\MyEducation\\MyCourses\\CodeInstitute\\DataAnalytics\\VSCode\\predictive-maintenance-project\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    858\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    859\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    860\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    861\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    862\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    870\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    871\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    872\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/train_FD001.txt'"
          ]
        }
      ],
      "source": [
        "# The Kaggle dataset files are space separated with trailing empty columns\n",
        "# The following function loads the FD001 files into pandas DataFrame\n",
        "# The files have no header row, so column names were added using the metadata provided on Kaggle\n",
        "\n",
        "def load_fd001(data_dir=\"data\"):\n",
        "    col_names = [\n",
        "        \"engine_id\", \"cycle\",\n",
        "        \"op_setting_1\", \"op_setting_2\", \"op_setting_3\"\n",
        "    ] + [f\"sensor_{i}\" for i in range(1, 22)]\n",
        "\n",
        "    df_train = pd.read_csv(\n",
        "        f\"{data_dir}/train_FD001.txt\",\n",
        "        sep=r\"\\s+\",\n",
        "        header=None,\n",
        "        names=col_names\n",
        "    )\n",
        "\n",
        "    df_test = pd.read_csv(\n",
        "        f\"{data_dir}/test_FD001.txt\",\n",
        "        sep=r\"\\s+\",\n",
        "        header=None,\n",
        "        names=col_names\n",
        "    )\n",
        "\n",
        "    df_rul = pd.read_csv(\n",
        "        f\"{data_dir}/RUL_FD001.txt\",\n",
        "        sep=r\"\\s+\",\n",
        "        header=None,\n",
        "        names=[\"RUL\"]\n",
        "    )\n",
        "\n",
        "    return df_train, df_test, df_rul\n",
        "\n",
        "df_train, df_test, df_rul = load_fd001()\n",
        "\n",
        "df_train.head(), df_test.head(), df_rul.head()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.head(), df_test.head(), df_rul.head()\n",
        "print(\"Training data shape:\", df_train.shape)\n",
        "print(\"Testing data shape:\", df_test.shape)\n",
        "print(\"RUL data shape:\", df_rul.shape)\n",
        "# Display first few rows of the training data\n",
        "print(df_train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Training data shape:\", df_train.shape)\n",
        "print(\"Testing data shape:\", df_test.shape)\n",
        "print(\"RUL data shape:\", df_rul.shape)\n",
        "# Display first few rows of the training data\n",
        "print(df_train.head())\n",
        "# Calculate RUL for training data\n",
        "rul_train = df_train.groupby(\"engine_id\")[\"cycle\"].max().reset_index()\n",
        "rul_train.columns = [\"engine_id\", \"max_cycle\"]\n",
        "df_train = df_train.merge(rul_train, on=\"engine_id\", how=\"left\")\n",
        "df_train[\"RUL\"] = df_train[\"max_cycle\"] - df_train[\"cycle\"]\n",
        "df_train.drop(\"max_cycle\", axis=1, inplace=True)\n",
        "print(\"Training data with RUL:\")\n",
        "print(df_train.head())\n",
        "# Prepare test data with RUL\n",
        "rul_test = df_test.groupby(\"engine_id\")[\"cycle\"].max().reset_index()\n",
        "rul_test.columns = [\"engine_id\", \"max_cycle\"]\n",
        "df_test = df_test.merge(rul_test, on=\"engine_id\", how=\"left\")\n",
        "df_test[\"RUL\"] = df_rul[\"RUL\"]\n",
        "df_test[\"RUL\"] = df_test[\"RUL\"] + (df_test[\"max_cycle\"] - df_test[\"cycle\"])\n",
        "df_test.drop(\"max_cycle\", axis=1, inplace=True)\n",
        "print(\"Testing data with RUL:\")\n",
        "print(df_test.head())\n",
        "# Feature selection: Drop columns with low variance or irrelevant features\n",
        "irrelevant_sensors = [\"sensor_4\", \"sensor_5\", \"sensor_10\", \"sensor_16\", \"sensor_18\", \"sensor_19\"]\n",
        "df_train.drop(columns=irrelevant_sensors, inplace=True)\n",
        "df_test.drop(columns=irrelevant_sensors, inplace=True)\n",
        "# Split training data into features and target\n",
        "X = df_train.drop(columns=[\"RUL\", \"engine_id\", \"cycle\"])\n",
        "y = df_train[\"RUL\"]\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(df_test.drop(columns=[\"RUL\", \"engine_id\", \"cycle\"]))\n",
        "# Train Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "# Validate the model\n",
        "y_val_pred = rf_model.predict(X_val)\n",
        "mse = mean_squared_error(y_val, y_val_pred)\n",
        "mae = mean_absolute_error(y_val, y_val_pred)\n",
        "print(f\"Validation MSE: {mse:.2f}\")\n",
        "print(f\"Validation MAE: {mae:.2f}\")\n",
        "# Test the model\n",
        "y_test_pred = rf_model.predict(X_test)\n",
        "print(\"Test predictions (first 10):\", y_test_pred[:10])\n",
        "# Visualize feature importance\n",
        "feature_importances = rf_model.feature_importances_\n",
        "features = X.columns\n",
        "importance_df = pd.DataFrame({\n",
        "    \"Feature\": features,\n",
        "    \"Importance\": feature_importances\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=\"Importance\", y=\"Feature\", data=importance_df)\n",
        "plt.title(\"Feature Importance from Random Forest Regressor\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exploratory Data Analysis - EDA\n",
        "\n",
        "* The following cells conduct a number of EDA steps to understand, clean and prepare the data for data analytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Training data shape:\", df_train.shape)\n",
        "print(\"Testing data shape:\", df_test.shape)\n",
        "print(\"RUL data shape:\", df_rul.shape)\n",
        "# Display first few rows of the training data\n",
        "print(df_train.head())\n",
        "# Display basic information about the training data\n",
        "print(\"Training Data Info:\")\n",
        "print(df_train.info())\n",
        "print(\"\\nTraining Data Head:\")\n",
        "print(df_train.head())\n",
        "print(\"\\nTraining Data Description:\")\n",
        "print(df_train.describe())\n",
        "print(\"\\nTraining Data Missing Values:\")\n",
        "print(df_train.isnull().sum())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "# Display basic information about the test data\n",
        "print(\"Test Data Info:\")\n",
        "print(df_test.info())\n",
        "print(\"\\nTest Data Head:\")\n",
        "print(df_test.head())\n",
        "print(\"\\nTest Data Description:\")\n",
        "print(df_test.describe())\n",
        "print(\"\\nTest Data Missing Values:\")\n",
        "print(df_test.isnull().sum())\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "# Display basic information about the RUL data\n",
        "print(\"RUL Data Info:\")\n",
        "print(df_rul.info())    \n",
        "print(\"\\nRUL Data Head:\")\n",
        "print(df_rul.head())\n",
        "print(\"\\nRUL Data Description:\")\n",
        "print(df_rul.describe())\n",
        "print(\"\\nRUL Data Missing Values:\")\n",
        "print(df_rul.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Visualisation\n",
        "\n",
        "* The following cells use a number of data visualisation libraries to plot a number of visualisations that attempt to answer a number of business questions or prove some of our hypotheses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Descriptive Analytics\n",
        "\n",
        "* The following cells conduct a number of descriptive analytics on the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Diagnostic Analytics\n",
        "\n",
        "* The following cells conduct a number of diagnostic analytics on the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Engineering, Selection and Scaling\n",
        "\n",
        "* The following cells conduct Feature Engineering, Feature Selection and Feature Scaling to prepare the data for predictive analytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predictive Analytics\n",
        "\n",
        "* The following cells perform predictive maintenance analytics on a NASA Turbofan Engine \n",
        "* Remaining Useful Live (RUL) is used as target for the model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\User\\\\Documents\\\\Sarkima\\\\Sarki\\\\MyEducation\\\\MyCourses\\\\CodeInstitute\\\\DataAnalytics\\\\VSCode\\\\predictive-maintenance-project\\\\jupyter_notebooks'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\User\\\\Documents\\\\Sarkima\\\\Sarki\\\\MyEducation\\\\MyCourses\\\\CodeInstitute\\\\DataAnalytics\\\\VSCode\\\\predictive-maintenance-project'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Section 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 1 content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 2 content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* You may add as many sections as you want, as long as it supports your project workflow.\n",
        "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In cases where you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  # create your folder here\n",
        "  # os.makedirs(name='')\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
